#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ImageProtectionBenchç»“æœå¯è§†åŒ–è„šæœ¬
æ”¯æŒæ”»å‡»å¯¹æ¯”çš„å¯è§†åŒ–
"""

import os
import json
import glob
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

def group_experiments_by_dataset(experiment_dirs):
    """æŒ‰æ•°æ®é›†å¯¹å®éªŒè¿›è¡Œåˆ†ç»„"""
    dataset_groups = {}
    
    for exp_dir in experiment_dirs:
        args_path = os.path.join(exp_dir, "results", "args.json")
        if not os.path.exists(args_path):
            print(f"âš ï¸ è·³è¿‡ç¼ºå°‘args.jsonçš„å®éªŒ: {os.path.basename(exp_dir)}")
            continue
            
        with open(args_path, 'r') as f:
            args = json.load(f)
        
        dataset = args.get('dataset', 'unknown')
        if dataset not in dataset_groups:
            dataset_groups[dataset] = []
        dataset_groups[dataset].append(exp_dir)
    
    return dataset_groups

def _extract_base_args(args_path):
    """æå–åŸºå‡†å‚æ•°"""
    with open(args_path, 'r') as f:
        args = json.load(f)
    
    return {
        'dataset': args.get('dataset', 'unknown'),
        'num_samples': args.get('num_samples', 'unknown'),
        'i2v_model': args.get('i2v_model', 'unknown'),
        'enable_attack': args.get('enable_attack', False),
        'attack_type': args.get('attack_type', None) if args.get('enable_attack', False) else None,
        'metrics': set(args.get('metrics', []))
    }

def _print_validation_baseline(base_args, context=""):
    """æ‰“å°éªŒè¯åŸºå‡†ä¿¡æ¯"""
    print(f"{context}éªŒè¯åŸºå‡†:")
    print(f"- æ ·æœ¬æ•°é‡: {base_args['num_samples']}")
    print(f"- I2Væ¨¡å‹: {base_args['i2v_model']}")
    print(f"- æ”»å‡»çŠ¶æ€: {'æœ‰æ”»å‡»' if base_args['enable_attack'] else 'æ— æ”»å‡»'}")
    if base_args['enable_attack']:
        print(f"- æ”»å‡»ç±»å‹: {base_args['attack_type']}")
    print(f"- è¯„ä¼°æŒ‡æ ‡: {', '.join(sorted(base_args['metrics']))}")

def _check_experiment_consistency(exp_dir, base_args, check_dataset=True):
    """æ£€æŸ¥å•ä¸ªå®éªŒçš„ä¸€è‡´æ€§"""
    args_path = os.path.join(exp_dir, "results", "args.json")
    results_path = os.path.join(exp_dir, "results", "benchmark_results.json")
    
    if not os.path.exists(args_path):
        return None, f"{os.path.basename(exp_dir)}: ç¼ºå°‘args.jsonæ–‡ä»¶"
        
    if not os.path.exists(results_path):
        return None, f"{os.path.basename(exp_dir)}: ç¼ºå°‘benchmark_results.jsonæ–‡ä»¶"
    
    # è¯»å–å®éªŒé…ç½®å’Œç»“æœ
    with open(args_path, 'r') as f:
        current_args = json.load(f)
    with open(results_path, 'r') as f:
        current_results = json.load(f)
    
    method_name = current_results.get('method', os.path.basename(exp_dir))
    issues = []
    
    # æ£€æŸ¥å…³é”®å‚æ•°ä¸€è‡´æ€§
    if check_dataset and current_args.get('dataset', 'unknown') != base_args['dataset']:
        issues.append(f"æ•°æ®é›†ä¸ä¸€è‡´ ({current_args.get('dataset', 'unknown')} vs {base_args['dataset']})")
    
    if current_args.get('num_samples', 'unknown') != base_args['num_samples']:
        issues.append(f"æ ·æœ¬æ•°é‡ä¸ä¸€è‡´ ({current_args.get('num_samples', 'unknown')} vs {base_args['num_samples']})")
    
    if current_args.get('i2v_model', 'unknown') != base_args['i2v_model']:
        issues.append(f"I2Væ¨¡å‹ä¸ä¸€è‡´ ({current_args.get('i2v_model', 'unknown')} vs {base_args['i2v_model']})")
    
    current_enable_attack = current_args.get('enable_attack', False)
    if current_enable_attack != base_args['enable_attack']:
        attack_status = "æœ‰æ”»å‡»" if current_enable_attack else "æ— æ”»å‡»"
        base_status = "æœ‰æ”»å‡»" if base_args['enable_attack'] else "æ— æ”»å‡»"
        issues.append(f"æ”»å‡»çŠ¶æ€ä¸ä¸€è‡´ ({attack_status} vs {base_status})")
    
    if base_args['enable_attack'] and current_enable_attack:
        current_attack_type = current_args.get('attack_type', None)
        if current_attack_type != base_args['attack_type']:
            issues.append(f"æ”»å‡»ç±»å‹ä¸ä¸€è‡´ ({current_attack_type} vs {base_args['attack_type']})")
    
    current_metrics = set(current_args.get('metrics', []))
    missing_core_metrics = base_args['metrics'] - current_metrics
    if missing_core_metrics:
        issues.append(f"ç¼ºå°‘æ ¸å¿ƒæŒ‡æ ‡ {missing_core_metrics}")
    
    return method_name, issues

def _check_method_duplicates_and_count(method_names):
    """æ£€æŸ¥æ–¹æ³•é‡å¤å’Œæ•°é‡"""
    issues = []
    unique_methods = set(method_names)
    
    if len(unique_methods) != len(method_names):
        method_counts = {}
        for method in method_names:
            method_counts[method] = method_counts.get(method, 0) + 1
        duplicates = [f"{method}({count}æ¬¡)" for method, count in method_counts.items() if count > 1]
        issues.append(f"é‡å¤çš„ä¿æŠ¤æ–¹æ³•: {', '.join(duplicates)}")
    
    if len(unique_methods) < 2:
        issues.append(f"æ–¹æ³•æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯” (å½“å‰åªæœ‰ {len(unique_methods)} ä¸ªä¸åŒæ–¹æ³•)")
    
    return issues

def _normalize_values_for_display(values, precision=3):
    """æ ‡å‡†åŒ–æ•°å€¼ç”¨äºæ˜¾ç¤ºï¼Œé¿å…ç²¾åº¦é—®é¢˜å¯¼è‡´çš„è§†è§‰å·®å¼‚"""
    if values is None or len(values) == 0:
        return values
    
    # å°†æ•°å€¼å››èˆäº”å…¥åˆ°æŒ‡å®šç²¾åº¦
    normalized = [round(float(v), precision) for v in values]
    
    # å¦‚æœæ‰€æœ‰å€¼éƒ½ç›¸åŒï¼Œç¡®ä¿å®ƒä»¬å®Œå…¨ä¸€è‡´
    if len(set(normalized)) == 1:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼ä½œä¸ºæ ‡å‡†å€¼
        standard_value = normalized[0]
        normalized = [standard_value] * len(normalized)
    
    return normalized

def validate_single_dataset_group(experiment_dirs, dataset_name):
    """éªŒè¯å•ä¸ªæ•°æ®é›†ç»„å†…å®éªŒçš„ä¸€è‡´æ€§"""
    if not experiment_dirs:
        return False, "æ²¡æœ‰æ‰¾åˆ°å®éªŒæ•°æ®"
    
    print(f"éªŒè¯æ•°æ®é›† {dataset_name} çš„ {len(experiment_dirs)} ä¸ªå®éªŒ...")
    
    # è¯»å–ç¬¬ä¸€ä¸ªå®éªŒçš„argsä½œä¸ºåŸºå‡†
    first_args_path = os.path.join(experiment_dirs[0], "results", "args.json")
    if not os.path.exists(first_args_path):
        return False, f"æœªæ‰¾åˆ°åŸºå‡†argsæ–‡ä»¶: {first_args_path}"
    
    base_args = _extract_base_args(first_args_path)
    _print_validation_baseline(base_args, f"æ•°æ®é›† {dataset_name}")
    
    # æ£€æŸ¥ç»„å†…å®éªŒçš„ä¸€è‡´æ€§
    inconsistent_experiments = []
    method_names = []
    
    for exp_dir in experiment_dirs:
        method_name, issues = _check_experiment_consistency(exp_dir, base_args, check_dataset=False)
        if method_name is None:
            inconsistent_experiments.append(issues)
            continue
        
        method_names.append(method_name)
        for issue in issues:
            inconsistent_experiments.append(f"{method_name}: {issue}")
    
    # æ£€æŸ¥æ–¹æ³•é‡å¤å’Œæ•°é‡
    method_issues = _check_method_duplicates_and_count(method_names)
    inconsistent_experiments.extend(method_issues)
    
    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    if inconsistent_experiments:
        print(f"\nâš ï¸ æ•°æ®é›† {dataset_name} å‘ç°ä¸€è‡´æ€§é—®é¢˜:")
        for issue in inconsistent_experiments:
            print(f"  - {issue}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è‡´å‘½é—®é¢˜
        fatal_issues = any("I2Væ¨¡å‹ä¸ä¸€è‡´" in issue or "æ–¹æ³•æ•°é‡ä¸è¶³" in issue 
                          for issue in inconsistent_experiments)
        
        if fatal_issues:
            print(f"\nâŒ æ•°æ®é›† {dataset_name} å‘ç°è‡´å‘½çš„ä¸€è‡´æ€§é—®é¢˜")
            return False, f"æ•°æ®é›† {dataset_name} å®éªŒé…ç½®å­˜åœ¨è‡´å‘½å·®å¼‚"
        else:
            print(f"\nâš ï¸ æ•°æ®é›† {dataset_name} å­˜åœ¨ä¸€è‡´æ€§é—®é¢˜ï¼Œä½†å¯ä»¥ç»§ç»­å¤„ç†")
            return True, f"æ•°æ®é›† {dataset_name} å‘ç° {len(inconsistent_experiments)} ä¸ªä¸€è‡´æ€§é—®é¢˜"
    else:
        print(f"âœ… æ•°æ®é›† {dataset_name} æ‰€æœ‰å®éªŒä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
        return True, f"æ•°æ®é›† {dataset_name} å®éªŒé…ç½®ä¸€è‡´"

def validate_batch_experiment_consistency(experiment_dirs):
    """éªŒè¯æ‰¹æ¬¡å®éªŒç»“æœçš„ä¸€è‡´æ€§ï¼ˆåŸºäºargs.jsonæ–‡ä»¶ï¼‰"""
    if not experiment_dirs:
        return False, "æ²¡æœ‰æ‰¾åˆ°å®éªŒæ•°æ®"
    
    print(f"éªŒè¯ {len(experiment_dirs)} ä¸ªå®éªŒçš„ä¸€è‡´æ€§...")
    
    # é¦–å…ˆæŒ‰æ•°æ®é›†åˆ†ç»„
    dataset_groups = group_experiments_by_dataset(experiment_dirs)
    
    if len(dataset_groups) > 1:
        print(f"âš ï¸ æ£€æµ‹åˆ° {len(dataset_groups)} ä¸ªä¸åŒçš„æ•°æ®é›†:")
        for dataset, dirs in dataset_groups.items():
            print(f"  - {dataset}: {len(dirs)} ä¸ªå®éªŒ")
        print("å°†å¯¹æ¯ä¸ªæ•°æ®é›†åˆ†åˆ«è¿›è¡ŒéªŒè¯å’Œå¯è§†åŒ–...")
        return True, f"æ£€æµ‹åˆ°å¤šæ•°æ®é›†ï¼Œå°†åˆ†ç»„å¤„ç† ({len(dataset_groups)} ä¸ªæ•°æ®é›†)"
    
    # å•æ•°æ®é›†çš„åŸæœ‰é€»è¾‘
    first_args_path = os.path.join(experiment_dirs[0], "results", "args.json")
    if not os.path.exists(first_args_path):
        return False, f"æœªæ‰¾åˆ°åŸºå‡†argsæ–‡ä»¶: {first_args_path}"
    
    base_args = _extract_base_args(first_args_path)
    _print_validation_baseline(base_args, "å®éªŒæ‰¹æ¬¡")
    
    # æ£€æŸ¥æ‰€æœ‰å®éªŒçš„ä¸€è‡´æ€§
    inconsistent_experiments = []
    method_names = []
    
    for exp_dir in experiment_dirs:
        method_name, issues = _check_experiment_consistency(exp_dir, base_args, check_dataset=True)
        if method_name is None:
            inconsistent_experiments.append(issues)
            continue
        
        method_names.append(method_name)
        for issue in issues:
            inconsistent_experiments.append(f"{method_name}: {issue}")
    
    # æ£€æŸ¥æ–¹æ³•é‡å¤å’Œæ•°é‡
    method_issues = _check_method_duplicates_and_count(method_names)
    inconsistent_experiments.extend(method_issues)
    
    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    if inconsistent_experiments:
        print(f"\nâš ï¸ å‘ç°å®éªŒæ‰¹æ¬¡ä¸€è‡´æ€§é—®é¢˜:")
        for issue in inconsistent_experiments:
            print(f"  - {issue}")
        
        # å¦‚æœé—®é¢˜ä¸æ˜¯è‡´å‘½çš„ï¼ˆå¦‚åªæ˜¯ç¼ºå°‘æŸäº›æŒ‡æ ‡ï¼‰ï¼Œè¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
        fatal_issues = any("æ•°æ®é›†ä¸ä¸€è‡´" in issue or "I2Væ¨¡å‹ä¸ä¸€è‡´" in issue or "æ–¹æ³•æ•°é‡ä¸è¶³" in issue 
                          for issue in inconsistent_experiments)
        
        if fatal_issues:
            print("\nâŒ å‘ç°è‡´å‘½çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆå¯¹æ¯”")
            return False, "å®éªŒé…ç½®å­˜åœ¨è‡´å‘½å·®å¼‚"
        else:
            while True:
                choice = input(f"\næ˜¯å¦ç»§ç»­ç»˜åˆ¶å›¾è¡¨? (y/n): ").strip().lower()
                if choice in ['y', 'yes']:
                    print("âš ï¸ ç»§ç»­å¤„ç†ï¼Œä½†è¯·æ³¨æ„ç»“æœå¯èƒ½ä¸å‡†ç¡®")
                    return True, f"å‘ç° {len(inconsistent_experiments)} ä¸ªä¸€è‡´æ€§é—®é¢˜ï¼Œä½†ç”¨æˆ·é€‰æ‹©ç»§ç»­"
                elif choice in ['n', 'no']:
                    return False, "ç”¨æˆ·é€‰æ‹©åœæ­¢å¤„ç†"
                else:
                    print("è¯·è¾“å…¥ y æˆ– n")
    else:
        print("âœ… æ‰€æœ‰å®éªŒæ‰¹æ¬¡ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
        return True, "å®éªŒæ‰¹æ¬¡é…ç½®ä¸€è‡´"


def validate_experiment_consistency(results_data):
    """éªŒè¯å®éªŒç»“æœçš„ä¸€è‡´æ€§ï¼ˆä¿ç•™åŸæœ‰æ¥å£ç”¨äºå…¼å®¹æ€§ï¼‰"""
    if not results_data:
        return False, "æ²¡æœ‰æ‰¾åˆ°å®éªŒæ•°æ®"
    
    print("âš ï¸ å»ºè®®ä½¿ç”¨ validate_batch_experiment_consistency è¿›è¡Œæ›´ç²¾ç¡®çš„éªŒè¯")
    return True, "ä½¿ç”¨ç®€åŒ–éªŒè¯"

def load_results():
    """åŠ è½½å®éªŒç»“æœ"""
    possible_paths = [
        "../outputs/*/results/benchmark_results.json",
        "outputs/*/results/benchmark_results.json",
        "./outputs/*/results/benchmark_results.json"
    ]
    
    json_files = []
    for pattern in possible_paths:
        files = glob.glob(pattern)
        if files:
            json_files = files
            break
    
    if not json_files:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•benchmark_results.jsonæ–‡ä»¶")
        return None, False
    
    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªå®éªŒç»“æœæ–‡ä»¶")
    
    # è¯»å–æ‰€æœ‰æ•°æ®å¹¶è¿›è¡ŒéªŒè¯
    all_results_data = []
    data = []
    has_attack = False
    
    for file_path in json_files:
        with open(file_path, 'r') as f:
            result = json.load(f)
            
            # æ·»åŠ timeæ•°æ®åˆ°ä¸»ç»“æœä¸­ï¼ˆç”¨äºéªŒè¯ï¼‰
            if 'time' in result:
                result.update(result['time'])
            
            all_results_data.append(result)
            
            # å‡†å¤‡DataFrameæ•°æ®
            row = {'method': result['method']}
            row.update(result['aggregated'])
            if 'time' in result:
                row.update(result['time'])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”»å‡»æ•°æ®
            if any('attacked_' in key for key in result['aggregated'].keys()):
                has_attack = True
            
            data.append(row)
    
    # éªŒè¯å®éªŒä¸€è‡´æ€§
    is_valid, message = validate_experiment_consistency(all_results_data)
    
    if not is_valid:
        print(f"å®éªŒéªŒè¯å¤±è´¥: {message}")
        return None, False
    
    print(f"âœ… éªŒè¯é€šè¿‡: {message}")
    return pd.DataFrame(data), has_attack

def plot_image_metrics(df, methods, has_attack, output_dir):
    """ç»˜åˆ¶å›¾åƒè´¨é‡æŒ‡æ ‡å¯¹æ¯”"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    metrics = ['psnr', 'ssim', 'lpips']
    metric_names = ['PSNR (dB)', 'SSIM', 'LPIPS']
    colors = ['#1f77b4', '#ff7f0e', '#d62728']
    
    x = np.arange(len(methods))
    width = 0.3
    
    for i, (metric, name) in enumerate(zip(metrics, metric_names)):
        ax = axes[i]
        
        protected_key = f'protected_{metric}'
        attacked_key = f'attacked_{metric}'
        
        # æ”¶é›†æ‰€æœ‰æ•°å€¼ç”¨äºè®¡ç®—çºµè½´èŒƒå›´
        all_values = []
        
        if has_attack and protected_key in df.columns and attacked_key in df.columns:
            # æ”»å‡»æ¨¡å¼ï¼šæ˜¾ç¤ºä¿æŠ¤å vs æ”»å‡»å
            protected_vals = _normalize_values_for_display(df[protected_key].values, precision=3)
            attacked_vals = _normalize_values_for_display(df[attacked_key].values, precision=3)
            
            ax.bar(x - width/2, protected_vals, width, label='Protected', alpha=0.8, color=colors[1])
            ax.bar(x + width/2, attacked_vals, width, label='Attacked', alpha=0.8, color=colors[2])
            
            all_values.extend(protected_vals)
            all_values.extend(attacked_vals)
        elif protected_key in df.columns:
            # å¸¸è§„æ¨¡å¼ï¼šåªæ˜¾ç¤ºä¿æŠ¤å
            protected_vals = _normalize_values_for_display(df[protected_key].values, precision=3)
            ax.bar(x, protected_vals, width, label='Protected', alpha=0.8, color=colors[1])
            
            all_values.extend(protected_vals)
        
        # è®¾ç½®åˆç†çš„çºµè½´èŒƒå›´ï¼Œé¿å…æ”¾å¤§å¾®å°å·®å¼‚
        if all_values:
            min_val = min(all_values)
            max_val = max(all_values)
            data_range = max_val - min_val
            
            if metric == 'psnr':
                # PSNR: é€šå¸¸åœ¨0-100ä¹‹é—´ï¼Œä½†å®é™…æ•°æ®èŒƒå›´å¯èƒ½æ›´å¹¿
                if data_range < 1.0:  # å·®å¼‚å¾ˆå°ï¼Œä½¿ç”¨å›ºå®šèŒƒå›´
                    center = (min_val + max_val) / 2
                    y_min = max(0, center - 5.0)  # å¢åŠ è¾¹è·
                    y_max = center + 5.0
                else:
                    # æ­£å¸¸å·®å¼‚ï¼Œä½¿ç”¨é€‚åº¦è¾¹è·ï¼Œä¸é™åˆ¶èŒƒå›´
                    margin = max(2.0, data_range * 0.15)
                    y_min = max(0, min_val - margin)
                    y_max = max_val + margin  # ç§»é™¤ä¸Šé™é™åˆ¶
                
                # è®¾ç½®åˆç†çš„åˆ»åº¦é—´éš”
                tick_step = max(2.0, (y_max - y_min) / 6)
                ax.set_yticks(np.arange(y_min, y_max + tick_step/2, tick_step))
                
            elif metric == 'ssim':
                # SSIM: é€šå¸¸åœ¨0-1.0ä¹‹é—´ï¼Œä½†å®é™…æ•°æ®å¯èƒ½æ›´ä½
                if data_range < 0.01:  # å·®å¼‚å¾ˆå°ï¼Œä½¿ç”¨å›ºå®šèŒƒå›´
                    center = (min_val + max_val) / 2
                    y_min = max(0.0, center - 0.1)  # å…è®¸æ›´ä½çš„SSIMå€¼
                    y_max = min(1.0, center + 0.1)
                else:
                    # æ­£å¸¸å·®å¼‚ï¼Œä½¿ç”¨é€‚åº¦è¾¹è·ï¼Œä¸é™åˆ¶æœ€å°å€¼
                    margin = max(0.05, data_range * 0.15)
                    y_min = max(0.0, min_val - margin)  # ç§»é™¤0.7çš„é™åˆ¶
                    y_max = min(1.0, max_val + margin)
                
                # è®¾ç½®åˆç†çš„åˆ»åº¦é—´éš”
                tick_step = max(0.05, (y_max - y_min) / 6)
                ax.set_yticks(np.arange(y_min, y_max + tick_step/2, tick_step))
                
            elif metric == 'lpips':
                # LPIPS: å€¼è¶Šå°è¶Šå¥½ï¼Œé€šå¸¸åœ¨0-1.0ä¹‹é—´ï¼Œä½†å®é™…æ•°æ®å¯èƒ½æ›´é«˜
                if data_range < 0.01:  # å·®å¼‚å¾ˆå°ï¼Œä½¿ç”¨å›ºå®šèŒƒå›´
                    center = (min_val + max_val) / 2
                    y_min = max(0, center - 0.1)
                    y_max = min(1.0, center + 0.1)  # å…è®¸æ›´é«˜çš„LPIPSå€¼
                else:
                    # æ­£å¸¸å·®å¼‚ï¼Œä½¿ç”¨é€‚åº¦è¾¹è·ï¼Œä¸é™åˆ¶æœ€å¤§å€¼
                    margin = max(0.05, data_range * 0.15)
                    y_min = max(0, min_val - margin)
                    y_max = min(1.0, max_val + margin)  # ç§»é™¤0.5çš„é™åˆ¶
                
                # è®¾ç½®åˆç†çš„åˆ»åº¦é—´éš”
                tick_step = max(0.05, (y_max - y_min) / 6)
                ax.set_yticks(np.arange(y_min, y_max + tick_step/2, tick_step))
            
            ax.set_ylim(y_min, y_max)
        
        ax.set_xlabel('Methods')
        ax.set_ylabel(name)
        ax.set_title(f'{name} Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(methods, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # ä¸ºæ•°å€¼æ·»åŠ æ ‡ç­¾æ˜¾ç¤º
        if has_attack and protected_key in df.columns and attacked_key in df.columns:
            for j, (p_val, a_val) in enumerate(zip(protected_vals, attacked_vals)):
                ax.text(j - width/2, p_val + (max_val - min_val) * 0.01, f'{p_val:.3f}', 
                       ha='center', va='bottom', fontsize=8, rotation=0)
                ax.text(j + width/2, a_val + (max_val - min_val) * 0.01, f'{a_val:.3f}', 
                       ha='center', va='bottom', fontsize=8, rotation=0)
        elif protected_key in df.columns:
            for j, p_val in enumerate(protected_vals):
                ax.text(j, p_val + (max_val - min_val) * 0.01, f'{p_val:.3f}', 
                       ha='center', va='bottom', fontsize=8, rotation=0)
    
    plt.tight_layout()
    filename = 'attack_image_metrics.png' if has_attack else 'image_metrics.png'
    plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"å›¾åƒè´¨é‡æŒ‡æ ‡å›¾å·²ä¿å­˜: {os.path.join(output_dir, filename)}")

def plot_clip_scores(df, methods, has_attack, output_dir):
    """ç»˜åˆ¶CLIPåˆ†æ•°å¯¹æ¯”"""
    fig, ax = plt.subplots(figsize=(12, 6))
    colors = ['#1f77b4', '#ff7f0e', '#d62728', '#2ca02c', '#ff1493']
    
    x = np.arange(len(methods))
    width = 0.3
    
    protected_key = 'protected_clip_score'
    attacked_key = 'attacked_clip_score'
    upper_bound_key = 'clip_upper_bound'
    lower_bound_key = 'clip_lower_bound'
    
    # æ”¶é›†æ‰€æœ‰æ•°å€¼ç”¨äºè®¡ç®—çºµè½´èŒƒå›´
    all_values = []
    
    # è·å–ç†è®ºä¸Šé™å’Œä¸‹é™ - ä½¿ç”¨å¹³å‡å€¼å¤„ç†å¤šä¸ªå®éªŒçš„ä¸åŒä¸Šä¸‹é™
    upper_bound = df[upper_bound_key].mean() if upper_bound_key in df.columns else None
    lower_bound = df[lower_bound_key].mean() if lower_bound_key in df.columns else None
    
    if has_attack and protected_key in df.columns and attacked_key in df.columns:
        # æ”»å‡»æ¨¡å¼ï¼šæ˜¾ç¤ºä¿æŠ¤å vs æ”»å‡»å
        protected_vals = _normalize_values_for_display(df[protected_key].values, precision=4)
        attacked_vals = _normalize_values_for_display(df[attacked_key].values, precision=4)
        
        ax.bar(x - width/2, protected_vals, width, label='Protected Video', alpha=0.8, color=colors[1])
        ax.bar(x + width/2, attacked_vals, width, label='Attacked Video', alpha=0.8, color=colors[2])
        
        all_values.extend(protected_vals)
        all_values.extend(attacked_vals)
    elif protected_key in df.columns:
        # å¸¸è§„æ¨¡å¼ï¼šåªæ˜¾ç¤ºä¿æŠ¤å
        protected_vals = _normalize_values_for_display(df[protected_key].values, precision=4)
        ax.bar(x, protected_vals, width, label='Protected Video', alpha=0.8, color=colors[1])
        
        all_values.extend(protected_vals)
    
    # æ·»åŠ ç†è®ºä¸Šé™å’Œä¸‹é™åˆ°all_valuesç”¨äºè®¡ç®—èŒƒå›´
    if upper_bound is not None:
        all_values.append(upper_bound)
    if lower_bound is not None:
        all_values.append(lower_bound)
    
    # ç»˜åˆ¶ç†è®ºä¸Šé™å’Œä¸‹é™çš„æ°´å¹³çº¿
    if upper_bound is not None:
        ax.axhline(y=upper_bound, color=colors[3], linestyle='--', linewidth=2, 
                  label=f'Theretical Upperbound (Self Comparison): {upper_bound:.4f}', alpha=0.8)
    if lower_bound is not None:
        ax.axhline(y=lower_bound, color=colors[4], linestyle='--', linewidth=2, 
                  label=f'Lowerbound (Random Comparison): {lower_bound:.4f}', alpha=0.8)
    
    # è®¾ç½®åˆç†çš„çºµè½´èŒƒå›´ï¼Œé¿å…æ”¾å¤§å¾®å°å·®å¼‚
    if all_values:
        min_val = min(all_values)
        max_val = max(all_values)
        data_range = max_val - min_val
        
        # CLIPåˆ†æ•°é€šå¸¸åœ¨0-1.0ä¹‹é—´
        if data_range < 0.01:  # å·®å¼‚å¾ˆå°ï¼Œä½¿ç”¨å›ºå®šèŒƒå›´
            center = (min_val + max_val) / 2
            y_min = max(0.0, center - 0.05)
            y_max = min(1.0, center + 0.05)
        else:
            # æ­£å¸¸å·®å¼‚ï¼Œä½¿ç”¨é€‚åº¦è¾¹è·
            margin = max(0.02, data_range * 0.1)
            y_min = max(0.0, min_val - margin)
            y_max = min(1.0, max_val + margin)
        
        # è®¾ç½®åˆç†çš„åˆ»åº¦é—´éš”
        tick_step = max(0.05, (y_max - y_min) / 6)
        ax.set_yticks(np.arange(y_min, y_max + tick_step/2, tick_step))
        ax.set_ylim(y_min, y_max)
        
        # ä¸ºæ•°å€¼æ·»åŠ æ ‡ç­¾æ˜¾ç¤º
        if has_attack and protected_key in df.columns and attacked_key in df.columns:
            for j, (p_val, a_val) in enumerate(zip(protected_vals, attacked_vals)):
                ax.text(j - width/2, p_val + (max_val - min_val) * 0.01, f'{p_val:.4f}', 
                       ha='center', va='bottom', fontsize=8, rotation=0)
                ax.text(j + width/2, a_val + (max_val - min_val) * 0.01, f'{a_val:.4f}', 
                       ha='center', va='bottom', fontsize=8, rotation=0)
        elif protected_key in df.columns:
            for j, p_val in enumerate(protected_vals):
                ax.text(j, p_val + (max_val - min_val) * 0.01, f'{p_val:.4f}', 
                       ha='center', va='bottom', fontsize=8, rotation=0)
    
    ax.set_xlabel('Methods')
    ax.set_ylabel('CLIP Score')
    ax.set_title('Video Semantic Similarity (CLIP Score) Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(methods, rotation=45)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    filename = 'attack_clip_scores.png' if has_attack else 'clip_scores.png'
    plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"CLIPåˆ†æ•°å›¾å·²ä¿å­˜: {os.path.join(output_dir, filename)}")

def plot_vbench_metrics(df, methods, has_attack, output_dir):
    """ç»˜åˆ¶VBenchæŒ‡æ ‡å¯¹æ¯”"""
    vbench_dims = ['subject_consistency', 'motion_smoothness', 'aesthetic_quality', 'imaging_quality']
    dim_labels = ['Subject Consistency', 'Motion Smoothness', 'Aesthetic Quality', 'Imaging Quality']
    
    # æ£€æŸ¥æ˜¯å¦æœ‰VBenchæ•°æ®
    vbench_available = any(f'original_{dim}' in df.columns or f'protected_{dim}' in df.columns 
                          for dim in vbench_dims)
    
    if not vbench_available:
        print("æœªå‘ç°VBenchæ•°æ®ï¼Œè·³è¿‡VBenchå›¾è¡¨ç”Ÿæˆ")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    colors = ['#1f77b4', '#ff7f0e', '#d62728']
    
    x = np.arange(len(methods))
    width = 0.3
    
    for i, (dim, label) in enumerate(zip(vbench_dims, dim_labels)):
        ax = axes[i]
        
        original_key = f'original_{dim}'
        protected_key = f'protected_{dim}'
        
        # æ”¶é›†æ‰€æœ‰æ•°å€¼ç”¨äºè®¡ç®—çºµè½´èŒƒå›´
        all_values = []
        
        if original_key in df.columns and protected_key in df.columns:
            # æ˜¾ç¤ºåŸå§‹ vs ä¿æŠ¤å
            original_vals = _normalize_values_for_display(df[original_key].values, precision=3)
            protected_vals = _normalize_values_for_display(df[protected_key].values, precision=3)
            
            ax.bar(x - width/2, original_vals, width, label='Original', alpha=0.8, color=colors[0])
            ax.bar(x + width/2, protected_vals, width, label='Protected', alpha=0.8, color=colors[1])
            
            all_values.extend(original_vals)
            all_values.extend(protected_vals)
        elif protected_key in df.columns:
            # åªæœ‰ä¿æŠ¤æ•°æ®
            protected_vals = _normalize_values_for_display(df[protected_key].values, precision=3)
            ax.bar(x, protected_vals, width, label='Protected', alpha=0.8, color=colors[1])
            
            all_values.extend(protected_vals)
        else:
            # æ— æ•°æ®
            ax.text(0.5, 0.5, 'No Data', transform=ax.transAxes, ha='center', va='center')
            continue
        
        # è®¾ç½®åˆç†çš„çºµè½´èŒƒå›´ï¼Œç‰¹åˆ«é’ˆå¯¹VBenchæŒ‡æ ‡ä¼˜åŒ–
        if all_values:
            min_val = min(all_values)
            max_val = max(all_values)
            data_range = max_val - min_val
            
            # VBenchæŒ‡æ ‡é€šå¸¸åœ¨0.3-1.0ä¹‹é—´ï¼Œä½¿ç”¨æ›´åˆç†çš„åŒºé—´è®¾ç½®
            if data_range < 0.05:  # å·®å¼‚å¾ˆå°ï¼Œä½¿ç”¨å›ºå®šèŒƒå›´é¿å…æ”¾å¤§å¾®å°å·®å¼‚
                center = (min_val + max_val) / 2
                y_min = max(0.3, center - 0.1)
                y_max = min(1.0, center + 0.1)
            elif data_range < 0.2:  # ä¸­ç­‰å·®å¼‚ï¼Œä½¿ç”¨é€‚åº¦è¾¹è·
                margin = max(0.05, data_range * 0.2)
                y_min = max(0.3, min_val - margin)
                y_max = min(1.0, max_val + margin)
            else:  # å·®å¼‚è¾ƒå¤§ï¼Œä½¿ç”¨æ­£å¸¸è¾¹è·
                margin = data_range * 0.1
                y_min = max(0.3, min_val - margin)
                y_max = min(1.0, max_val + margin)
            
            # è®¾ç½®åˆç†çš„åˆ»åº¦é—´éš”ï¼Œé¿å…è¿‡å¯†
            tick_step = max(0.1, (y_max - y_min) / 4)
            ax.set_yticks(np.arange(y_min, y_max + tick_step/2, tick_step))
            ax.set_ylim(y_min, y_max)
            
            # ä¸ºæ•°å€¼æ·»åŠ æ ‡ç­¾æ˜¾ç¤º
            if original_key in df.columns and protected_key in df.columns:
                for j, (o_val, p_val) in enumerate(zip(original_vals, protected_vals)):
                    ax.text(j - width/2, o_val + (max_val - min_val) * 0.01, f'{o_val:.3f}', 
                           ha='center', va='bottom', fontsize=8, rotation=0)
                    ax.text(j + width/2, p_val + (max_val - min_val) * 0.01, f'{p_val:.3f}', 
                           ha='center', va='bottom', fontsize=8, rotation=0)
            elif protected_key in df.columns:
                for j, p_val in enumerate(protected_vals):
                    ax.text(j, p_val + (max_val - min_val) * 0.01, f'{p_val:.3f}', 
                           ha='center', va='bottom', fontsize=8, rotation=0)
        
        ax.set_xlabel('Methods')
        ax.set_ylabel('Score')
        ax.set_title(f'{label}')
        ax.set_xticks(x)
        ax.set_xticklabels(methods, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'vbench_metrics.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"VBenchæŒ‡æ ‡å›¾å·²ä¿å­˜: {os.path.join(output_dir, 'vbench_metrics.png')}")

def plot_attack_effectiveness(df, methods, output_dir):
    """ç»˜åˆ¶æ”»å‡»æ•ˆæœåˆ†æï¼ˆä»…åœ¨æœ‰æ”»å‡»æ•°æ®æ—¶ï¼‰"""
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ”»å‡»æ•°æ®
    has_psnr_attack = 'protected_psnr' in df.columns and 'attacked_psnr' in df.columns
    has_clip_attack = 'protected_clip_score' in df.columns and 'attacked_clip_score' in df.columns
    
    if not (has_psnr_attack or has_clip_attack):
        print("æ”»å‡»æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æ”»å‡»æ•ˆæœåˆ†æå›¾")
        return
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    colors = ['#d62728', '#2ca02c']
    
    if has_psnr_attack:
        # å·¦å›¾ï¼šPSNRæ”»å‡»æŸå¤±
        ax1 = axes[0]
        psnr_loss = df['protected_psnr'].values - df['attacked_psnr'].values
        
        bars = ax1.bar(methods, psnr_loss, alpha=0.7, color=colors[0])
        ax1.set_xlabel('Methods')
        ax1.set_ylabel('PSNR Loss (dB)')
        ax1.set_title('Attack Impact on Image Quality')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, loss in zip(bars, psnr_loss):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{loss:.1f}', ha='center', va='bottom')
    
    if has_clip_attack:
        # å³å›¾ï¼šCLIPé²æ£’æ€§
        ax2 = axes[1]
        clip_robustness = df['attacked_clip_score'].values / df['protected_clip_score'].values * 100
        
        bars = ax2.bar(methods, clip_robustness, alpha=0.7, color=colors[1])
        ax2.axhline(y=100, color='red', linestyle='--', alpha=0.7, label='No Impact')
        ax2.set_xlabel('Methods')
        ax2.set_ylabel('Robustness (%)')
        ax2.set_title('Attack Robustness (Higher is Better)')
        ax2.tick_params(axis='x', rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rob in zip(bars, clip_robustness):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 2,
                    f'{rob:.1f}%', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'attack_effectiveness.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"æ”»å‡»æ•ˆæœåˆ†æå›¾å·²ä¿å­˜: {os.path.join(output_dir, 'attack_effectiveness.png')}")

def plot_time_metrics(df, methods, output_dir):
    """ç»˜åˆ¶æ—¶é—´æŒ‡æ ‡å¯¹æ¯”"""
    if 'time_per_image' not in df.columns:
        print("æœªæ‰¾åˆ°æ—¶é—´æŒ‡æ ‡æ•°æ®ï¼Œè·³è¿‡æ—¶é—´å›¾è¡¨ç”Ÿæˆ")
        return
        
    fig, ax = plt.subplots(figsize=(12, 6))
    colors = ['#1f77b4', '#ff7f0e', '#d62728']
    
    x = np.arange(len(methods))
    width = 0.3
    
    # å°†æ—¶é—´è½¬æ¢ä¸ºæ¯«ç§’
    time_ms = df['time_per_image'].values * 1000
    
    bars = ax.bar(x, time_ms, width, label='Time per Image', alpha=0.8, color=colors[0])
    
    # è®¾ç½®æ›´ç²¾ç»†çš„çºµè½´èŒƒå›´
    min_time = min(time_ms)
    max_time = max(time_ms)
    
    # é’ˆå¯¹æ—¶é—´å·®å¼‚å¾ˆå¤§çš„æƒ…å†µï¼Œç‰¹æ®Šå¤„ç†çºµè½´èŒƒå›´
    # æ£€æŸ¥æ˜¯å¦æœ‰æå°å€¼ï¼ˆå¦‚RandomNoiseï¼‰å’Œæå¤§å€¼å·®å¼‚å¾ˆå¤§çš„æƒ…å†µ
    time_ratio = max_time / min_time if min_time > 0 else float('inf')
    
    if time_ratio > 100:  # æ—¶é—´å·®å¼‚è¶…è¿‡100å€
        # ä½¿ç”¨å¯¹æ•°åˆ»åº¦æ›´åˆé€‚ï¼Œä½†è¿™é‡Œç”¨åˆ†æ®µçº¿æ€§å¤„ç†
        print(f"æ£€æµ‹åˆ°æ—¶é—´å·®å¼‚å¾ˆå¤§: æœ€å°{min_time:.1f}ms, æœ€å¤§{max_time:.1f}ms (æ¯”ç‡: {time_ratio:.1f})")
        
        # ä¸ºäº†è®©å°å€¼ä¹Ÿèƒ½çœ‹è§ï¼Œå‡å°‘ä¸‹è¾¹è·
        y_min = min_time * 0.9  # åªä¿ç•™10%çš„ä¸‹è¾¹è·
        y_max = max_time + (max_time - min_time) * 0.1
    else:
        # æ­£å¸¸æƒ…å†µçš„è¾¹è·å¤„ç†
        margin = (max_time - min_time) * 0.1
        y_min = max(0, min_time - margin)
        y_max = max_time + margin
    
    # æ ¹æ®æ—¶é—´èŒƒå›´é€‰æ‹©æ›´ç¨€ç–çš„åˆ»åº¦é—´éš”
    time_range = y_max - y_min
    if time_range <= 0:  # é˜²æ­¢èŒƒå›´ä¸º0æˆ–è´Ÿæ•°
        time_range = 1
    if time_range < 5:
        tick_step = 1
    elif time_range < 20:
        tick_step = 5
    elif time_range < 100:
        tick_step = 20
    elif time_range < 500:
        tick_step = 100
    elif time_range < 2000:
        tick_step = 500
    elif time_range < 10000:
        tick_step = 2000
    else:
        tick_step = 5000
    
    # è®¡ç®—åˆé€‚çš„åˆ»åº¦èµ·å§‹ç‚¹
    tick_start = int(y_min // tick_step) * tick_step
    
    # è®¾ç½®ç¨€ç–çš„åˆ»åº¦ï¼Œæœ€å¤š5ä¸ªåˆ»åº¦
    ticks = np.arange(tick_start, y_max + tick_step, tick_step)
    ticks = ticks[ticks >= y_min]
    
    # ä¸¥æ ¼é™åˆ¶æœ€å¤§åˆ»åº¦æ•°é‡ä¸º5ä¸ª
    if len(ticks) > 5:
        # åŠ¨æ€è°ƒæ•´é—´éš”ä»¥ç¡®ä¿æœ€å¤š5ä¸ªåˆ»åº¦
        target_tick_count = 4
        range_diff = y_max - y_min
        if range_diff == 0:
            new_tick_step = 1  # é˜²æ­¢é™¤é›¶é”™è¯¯
        else:
            new_tick_step = range_diff / target_tick_count
        
        # å°†tick_stepè°ƒæ•´ä¸ºè¾ƒä¸ºæ•´æ•°çš„å€¼
        if new_tick_step < 10:
            tick_step = max(1, int(new_tick_step))
        elif new_tick_step < 100:
            tick_step = int(new_tick_step / 10) * 10
        else:
            tick_step = int(new_tick_step / 100) * 100
            
        tick_start = int(y_min // tick_step) * tick_step
        ticks = np.arange(tick_start, y_max + tick_step, tick_step)
        ticks = ticks[ticks >= y_min]
        
        # å¦‚æœè¿˜æ˜¯å¤ªå¤šï¼Œå†æ¬¡å‡å°‘
        if len(ticks) > 5:
            ticks = ticks[::2]  # å–æ¯éš”ä¸€ä¸ªåˆ»åº¦
    
    ax.set_yticks(ticks)
    ax.set_ylim(y_min, y_max)
    
    # è®¾ç½®çºµè½´åˆ»åº¦æ ‡ç­¾æ ¼å¼ï¼Œé¿å…æ˜¾ç¤ºè¿‡é•¿çš„æ•°å­—
    def format_time_label(x, pos):
        if x < 1000:
            return f'{x:.0f}ms'
        else:
            return f'{x/1000:.1f}s'
    
    from matplotlib.ticker import FuncFormatter
    ax.yaxis.set_major_formatter(FuncFormatter(format_time_label))
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾ï¼Œç‰¹æ®Šå¤„ç†æå°å€¼
    for i, (bar, time) in enumerate(zip(bars, time_ms)):
        height = bar.get_height()
        
        # æ ¼å¼åŒ–æ ‡ç­¾
        if time < 1000:
            label = f'{time:.1f}ms'
        else:
            label = f'{time/1000:.2f}s'
        
        # å¯¹äºæå°çš„æŸ±å­ï¼Œæ ‡ç­¾æ”¾åœ¨ä¸Šæ–¹æ›´é«˜çš„ä½ç½®ï¼Œç¡®ä¿å¯è§
        label_height = height + (y_max - y_min) * 0.02
        
        # å¦‚æœæŸ±å­å¤ªå°ï¼ˆå°äºæ€»é«˜åº¦çš„5%ï¼‰ï¼Œå°†æ ‡ç­¾æ”¾åœ¨å›¾è¡¨ä¸Šæ–¹
        if height < (y_max - y_min) * 0.05:
            label_height = height + (y_max - y_min) * 0.05
            print(f"æ–¹æ³• {methods[i]} çš„æ—¶é—´å¾ˆå° ({time:.1f}ms)ï¼Œè°ƒæ•´æ ‡ç­¾ä½ç½®")
        
        ax.text(bar.get_x() + bar.get_width()/2., label_height,
                label, ha='center', va='bottom', fontsize=8, rotation=0)
    
    # ç¡®ä¿æå°çš„æŸ±å­ä¹Ÿèƒ½çœ‹è§ - è®¾ç½®æœ€å°æŸ±å­é«˜åº¦
    min_visible_height = (y_max - y_min) * 0.02  # è®¾ç½®æœ€å°å¯è§é«˜åº¦ä¸º2%
    for i, (bar, time) in enumerate(zip(bars, time_ms)):
        if bar.get_height() < min_visible_height:
            # ä¸ºæå°çš„æŸ±å­æ·»åŠ ä¸€ä¸ªåŸºç¡€é«˜åº¦ï¼Œä½†ä¿æŒæ•°å€¼æ ‡ç­¾æ­£ç¡®
            bar.set_height(min_visible_height)
            print(f"ä¸ºæ–¹æ³• {methods[i]} è®¾ç½®æœ€å°å¯è§é«˜åº¦")
    
    ax.set_xlabel('Methods')
    ax.set_ylabel('Time per Image (ms)')
    ax.set_title('Processing Time Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(methods, rotation=45)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'time_metrics.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"æ—¶é—´æŒ‡æ ‡å›¾å·²ä¿å­˜: {os.path.join(output_dir, 'time_metrics.png')}")

def generate_dataset_group_visualizations(experiment_dirs, dataset_name, output_dir):
    """ä¸ºå•ä¸ªæ•°æ®é›†ç»„ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    try:
        print(f"\nä¸ºæ•°æ®é›† {dataset_name} ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è¯»å–æ•°æ®é›†ç»„çš„æ‰€æœ‰æ•°æ®
        data = []
        has_attack = False
        
        for exp_dir in experiment_dirs:
            results_path = os.path.join(exp_dir, "results", "benchmark_results.json")
            if not os.path.exists(results_path):
                print(f"âš ï¸ è·³è¿‡ç¼ºå°‘ç»“æœæ–‡ä»¶çš„å®éªŒ: {os.path.basename(exp_dir)}")
                continue
                
            with open(results_path, 'r') as f:
                result = json.load(f)
                
                # å‡†å¤‡DataFrameæ•°æ®
                row = {'method': result['method']}
                row.update(result['aggregated'])
                if 'time' in result:
                    row.update(result['time'])
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ”»å‡»æ•°æ®
                if any('attacked_' in key for key in result['aggregated'].keys()):
                    has_attack = True
                
                data.append(row)
        
        if not data:
            print(f"âŒ æ•°æ®é›† {dataset_name} æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒæ•°æ®")
            return False
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(data)
        methods = df['method'].tolist()
        
        print(f"æ•°æ®é›† {dataset_name}: {len(methods)} ä¸ªæ–¹æ³• - {methods}")
        print(f"åŒ…å«æ”»å‡»æ•°æ®: {has_attack}")
        
        # åˆ›å»ºæ•°æ®é›†ç‰¹å®šçš„è¾“å‡ºç›®å½•
        dataset_output_dir = os.path.join(output_dir, dataset_name)
        os.makedirs(dataset_output_dir, exist_ok=True)
        
        # ç”Ÿæˆå›¾è¡¨
        plot_image_metrics(df, methods, has_attack, dataset_output_dir)
        plot_clip_scores(df, methods, has_attack, dataset_output_dir)
        plot_vbench_metrics(df, methods, has_attack, dataset_output_dir)
        plot_time_metrics(df, methods, dataset_output_dir)
        
        if has_attack:
            plot_attack_effectiveness(df, methods, dataset_output_dir)
        
        print(f"âœ… æ•°æ®é›† {dataset_name} å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ: {dataset_output_dir}")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›† {dataset_name} ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

def _load_experiment_data(experiment_dirs):
    """ä»å®éªŒç›®å½•åŠ è½½æ•°æ®"""
    data = []
    has_attack = False
    
    for exp_dir in experiment_dirs:
        results_path = os.path.join(exp_dir, "results", "benchmark_results.json")
        if not os.path.exists(results_path):
            print(f"âš ï¸ è·³è¿‡ç¼ºå°‘ç»“æœæ–‡ä»¶çš„å®éªŒ: {os.path.basename(exp_dir)}")
            continue
            
        with open(results_path, 'r') as f:
            result = json.load(f)
            
        # å‡†å¤‡DataFrameæ•°æ®
        row = {'method': result['method']}
        row.update(result['aggregated'])
        if 'time' in result:
            row.update(result['time'])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ”»å‡»æ•°æ®
        if any('attacked_' in key for key in result['aggregated'].keys()):
            has_attack = True
        
        data.append(row)
    
    return data, has_attack

def _setup_matplotlib_style():
    """è®¾ç½®matplotlibé£æ ¼"""
    plt.style.use('default')
    plt.rcParams['font.size'] = 10

def _generate_plots(df, methods, has_attack, output_dir):
    """ç”Ÿæˆæ‰€æœ‰å›¾è¡¨"""
    plot_image_metrics(df, methods, has_attack, output_dir)
    plot_clip_scores(df, methods, has_attack, output_dir)
    plot_vbench_metrics(df, methods, has_attack, output_dir)
    plot_time_metrics(df, methods, output_dir)
    
    if has_attack:
        plot_attack_effectiveness(df, methods, output_dir)

def generate_batch_visualizations(output_base_dir: str = "outputs", output_dir: str = None) -> bool:
    """
    ä¸ºä¸€æ‰¹å®éªŒç»“æœç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨ï¼Œæ”¯æŒå¤šæ•°æ®é›†åˆ†ç»„å¤„ç†
    
    Args:
        output_base_dir: è¾“å‡ºåŸºç›®å½•ï¼ŒåŒ…å«å¤šä¸ªå®éªŒæ–‡ä»¶å¤¹
        output_dir: å¯è§†åŒ–å›¾è¡¨è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºoutput_base_dir/comparison_chartsï¼‰
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸç”Ÿæˆå›¾è¡¨
    """
    try:
        # æœç´¢æ‰€æœ‰å®éªŒç›®å½•
        experiment_dirs = []
        for item in os.listdir(output_base_dir):
            item_path = os.path.join(output_base_dir, item)
            if os.path.isdir(item_path) and item != "comparison_charts":
                # æ£€æŸ¥æ˜¯å¦æœ‰resultsç›®å½•å’Œå¿…è¦æ–‡ä»¶
                results_dir = os.path.join(item_path, "results")
                if os.path.exists(results_dir):
                    benchmark_results = os.path.join(results_dir, "benchmark_results.json")
                    args_file = os.path.join(results_dir, "args.json")
                    if os.path.exists(benchmark_results) and os.path.exists(args_file):
                        experiment_dirs.append(item_path)
        
        if not experiment_dirs:
            print(f"åœ¨ {output_base_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒç›®å½•")
            return False
        
        print(f"æ‰¾åˆ° {len(experiment_dirs)} ä¸ªå®éªŒç›®å½•")
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = os.path.join(output_base_dir, 'comparison_charts')
        
        os.makedirs(output_dir, exist_ok=True)
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„
        dataset_groups = group_experiments_by_dataset(experiment_dirs)
        
        if len(dataset_groups) > 1:
            # å¤šæ•°æ®é›†æƒ…å†µï¼šä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆå•ç‹¬çš„å›¾è¡¨
            print(f"\næ£€æµ‹åˆ° {len(dataset_groups)} ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œå°†åˆ†åˆ«ç”Ÿæˆå›¾è¡¨:")
            
            success_count = 0
            total_datasets = len(dataset_groups)
            
            _setup_matplotlib_style()
            
            for dataset_name, dirs in dataset_groups.items():
                print(f"\n{'-'*40}")
                print(f"å¤„ç†æ•°æ®é›†: {dataset_name}")
                print(f"{'-'*40}")
                
                # éªŒè¯æ•°æ®é›†ç»„å†…ä¸€è‡´æ€§
                is_valid, message = validate_single_dataset_group(dirs, dataset_name)
                if not is_valid:
                    print(f"âŒ æ•°æ®é›† {dataset_name} éªŒè¯å¤±è´¥: {message}")
                    continue
                
                # ç”Ÿæˆæ•°æ®é›†ç»„çš„å¯è§†åŒ–å›¾è¡¨
                if generate_dataset_group_visualizations(dirs, dataset_name, output_dir):
                    success_count += 1
            
            if success_count > 0:
                print(f"\nğŸ‰ å¤šæ•°æ®é›†æ‰¹é‡å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ!")
                print(f"ğŸ“ å›¾è¡¨ä¿å­˜ä½ç½®: {os.path.abspath(output_dir)}")
                print(f"âœ… æˆåŠŸå¤„ç† {success_count}/{total_datasets} ä¸ªæ•°æ®é›†")
                
                # æ˜¾ç¤ºç”Ÿæˆçš„ç›®å½•ç»“æ„
                print(f"\nç”Ÿæˆçš„æ•°æ®é›†å›¾è¡¨ç›®å½•:")
                for dataset_name in dataset_groups.keys():
                    dataset_dir = os.path.join(output_dir, dataset_name)
                    if os.path.exists(dataset_dir):
                        print(f"  - {dataset_name}/")
                        print(f"    â”œâ”€â”€ image_metrics.png")
                        print(f"    â”œâ”€â”€ clip_scores.png")
                        print(f"    â”œâ”€â”€ vbench_metrics.png")
                        print(f"    â””â”€â”€ time_metrics.png")
                
                return success_count == total_datasets
            else:
                print(f"âŒ æ‰€æœ‰æ•°æ®é›†å¤„ç†éƒ½å¤±è´¥äº†")
                return False
        
        else:
            # å•æ•°æ®é›†æƒ…å†µ
            dataset_name = list(dataset_groups.keys())[0]
            print(f"å•æ•°æ®é›†æ¨¡å¼: {dataset_name}")
            
            # åŠ è½½æ•°æ®
            data, has_attack = _load_experiment_data(experiment_dirs)
            
            if not data:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒæ•°æ®")
                return False
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data)
            methods = df['method'].tolist()
            
            _setup_matplotlib_style()
            
            print(f"æ£€æµ‹åˆ° {len(methods)} ä¸ªæ–¹æ³•: {methods}")
            print(f"åŒ…å«æ”»å‡»æ•°æ®: {has_attack}")
            print(f"å¯¹æ¯”å›¾è¡¨è¾“å‡ºç›®å½•: {output_dir}")
            
            # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
            _generate_plots(df, methods, has_attack, output_dir)
            
            print(f"å•æ•°æ®é›†å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ: {output_dir}")
            return True
        
    except Exception as e:
        print(f"ç”Ÿæˆæ‰¹é‡å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """
    ä¸»å‡½æ•°ï¼šéªŒè¯åŒä¸€æ‰¹æ¬¡å®éªŒå¹¶ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨
    """
    import datetime
    
    print("="*60)
    print("ImageProtectionBench æ‰¹æ¬¡å®éªŒå¯¹æ¯”å¯è§†åŒ–å·¥å…·")
    print("="*60)
    
    # æœç´¢å®éªŒç›®å½•
    output_base_dir = "outputs"
    if not os.path.exists(output_base_dir):
        print(f"âŒ æœªæ‰¾åˆ°è¾“å‡ºç›®å½•: {output_base_dir}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰å®éªŒç›®å½•
    experiment_dirs = []
    for item in os.listdir(output_base_dir):
        item_path = os.path.join(output_base_dir, item)
        if os.path.isdir(item_path) and item != "comparison_charts":
            # æ£€æŸ¥æ˜¯å¦æœ‰resultsç›®å½•å’Œå¿…è¦æ–‡ä»¶
            results_dir = os.path.join(item_path, "results")
            if os.path.exists(results_dir):
                benchmark_results = os.path.join(results_dir, "benchmark_results.json")
                args_file = os.path.join(results_dir, "args.json")
                if os.path.exists(benchmark_results) and os.path.exists(args_file):
                    experiment_dirs.append(item_path)
    
    if not experiment_dirs:
        print(f"âŒ åœ¨ {output_base_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒç›®å½•")
        print("å®éªŒç›®å½•åº”åŒ…å« results/benchmark_results.json å’Œ results/args.json æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(experiment_dirs)} ä¸ªå®éªŒç›®å½•:")
    for exp_dir in experiment_dirs:
        print(f"  - {os.path.basename(exp_dir)}")
    
    # æŒ‰æ•°æ®é›†åˆ†ç»„å¹¶æ˜¾ç¤ºä¿¡æ¯
    print(f"\n{'-'*40}")
    print("åˆ†æå®éªŒæ•°æ®é›†åˆ†å¸ƒ...")
    print(f"{'-'*40}")
    
    dataset_groups = group_experiments_by_dataset(experiment_dirs)
    
    print(f"æ£€æµ‹åˆ° {len(dataset_groups)} ä¸ªæ•°æ®é›†:")
    for dataset_name, dirs in dataset_groups.items():
        print(f"  - {dataset_name}: {len(dirs)} ä¸ªå®éªŒ")
        for exp_dir in dirs:
            print(f"    â””â”€â”€ {os.path.basename(exp_dir)}")
    
    # éªŒè¯å®éªŒæ‰¹æ¬¡ä¸€è‡´æ€§ï¼ˆå·²å†…ç½®æ•°æ®é›†åˆ†ç»„å¤„ç†ï¼‰
    print(f"\n{'-'*40}")
    print("éªŒè¯å®éªŒä¸€è‡´æ€§...")
    print(f"{'-'*40}")
    
    is_valid, message = validate_batch_experiment_consistency(experiment_dirs)
    
    if not is_valid:
        print(f"\nâŒ éªŒè¯å¤±è´¥: {message}")
        print("æ— æ³•ç”Ÿæˆå¯¹æ¯”å›¾è¡¨")
        return
    
    print(f"\nâœ… éªŒè¯é€šè¿‡: {message}")
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    if len(dataset_groups) > 1:
        output_dir = os.path.join("figs", f"multi_dataset_comparison_{timestamp}")
    else:
        dataset_name = list(dataset_groups.keys())[0]
        output_dir = os.path.join("figs", f"{dataset_name}_comparison_{timestamp}")
    
    print(f"\n{'-'*40}")
    if len(dataset_groups) > 1:
        print("ç”Ÿæˆå¤šæ•°æ®é›†åˆ†ç»„å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨...")
    else:
        print("ç”Ÿæˆå•æ•°æ®é›†å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨...")
    print(f"{'-'*40}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    success = generate_batch_visualizations(output_base_dir, output_dir)
    
    if success:
        if len(dataset_groups) > 1:
            print(f"\nğŸ‰ å¤šæ•°æ®é›†åˆ†ç»„å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨ç”ŸæˆæˆåŠŸ!")
        else:
            print(f"\nğŸ‰ å•æ•°æ®é›†å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨ç”ŸæˆæˆåŠŸ!")
        
        print(f"ğŸ“ å›¾è¡¨ä¿å­˜ä½ç½®: {os.path.abspath(output_dir)}")
        
        if len(dataset_groups) > 1:
            print(f"\nä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆçš„å›¾è¡¨ç›®å½•:")
            for dataset_name in dataset_groups.keys():
                print(f"  - {dataset_name}/")
                print(f"    â”œâ”€â”€ image_metrics.png: å›¾åƒè´¨é‡æŒ‡æ ‡å¯¹æ¯”")
                print(f"    â”œâ”€â”€ clip_scores.png: CLIPè¯­ä¹‰ç›¸ä¼¼åº¦å¯¹æ¯”")
                print(f"    â”œâ”€â”€ vbench_metrics.png: VBenchè§†é¢‘è´¨é‡æŒ‡æ ‡å¯¹æ¯”")
                print(f"    â”œâ”€â”€ time_metrics.png: å¤„ç†æ—¶é—´å¯¹æ¯”")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ”»å‡»æ•°æ®
                has_attack = False
                for exp_dir in dataset_groups[dataset_name]:
                    results_path = os.path.join(exp_dir, "results", "benchmark_results.json")
                    if os.path.exists(results_path):
                        with open(results_path, 'r') as f:
                            result = json.load(f)
                        if any('attacked_' in key for key in result['aggregated'].keys()):
                            has_attack = True
                            break
                
                if has_attack:
                    print(f"    â””â”€â”€ attack_effectiveness.png: æ”»å‡»æ•ˆæœåˆ†æ")
                else:
                    print(f"    â””â”€â”€ (æ— æ”»å‡»æ•°æ®)")
        else:
            print(f"\nç”Ÿæˆçš„å›¾è¡¨åŒ…æ‹¬:")
            print(f"  - image_metrics.png: å›¾åƒè´¨é‡æŒ‡æ ‡å¯¹æ¯”")
            print(f"  - clip_scores.png: CLIPè¯­ä¹‰ç›¸ä¼¼åº¦å¯¹æ¯”")
            print(f"  - vbench_metrics.png: VBenchè§†é¢‘è´¨é‡æŒ‡æ ‡å¯¹æ¯”")
            print(f"  - time_metrics.png: å¤„ç†æ—¶é—´å¯¹æ¯”")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”»å‡»æ•°æ®
            pattern = os.path.join(output_base_dir, "*/results/benchmark_results.json")
            json_files = glob.glob(pattern)
            has_attack = False
            for file_path in json_files:
                with open(file_path, 'r') as f:
                    result = json.load(f)
                if any('attacked_' in key for key in result['aggregated'].keys()):
                    has_attack = True
                    break
            
            if has_attack:
                print(f"  - attack_effectiveness.png: æ”»å‡»æ•ˆæœåˆ†æ")
            
    else:
        print(f"\nâŒ å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå¤±è´¥")
        
    print(f"\n{'='*60}")


if __name__ == "__main__":
    main()